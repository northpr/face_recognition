{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m paths\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mface_recognition\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imutils'"
     ]
    }
   ],
   "source": [
    "from imutils import paths\n",
    "import face_recognition\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"dataset\"] = \"../data/for_model/train/\"\n",
    "args[\"enconding\"] = \"enconding.pickle\"\n",
    "args[\"detection_method\"] = \"cnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create facial embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 12.76% | Encoded images: 349/2728 | Name: Chris_Evanser\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m img_rgb \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(img, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Detect the (x,y) - coordinates to the bounding boxes\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m boxes \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39;49mface_locations(img_rgb, model\u001b[39m=\u001b[39;49margs[\u001b[39m\"\u001b[39;49m\u001b[39mdetection_method\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     14\u001b[0m \u001b[39m# Compute the facial embedding for the face\u001b[39;00m\n\u001b[1;32m     15\u001b[0m encodings \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39mface_encodings(img_rgb, boxes)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/face_recognition/lib/python3.10/site-packages/face_recognition/api.py:119\u001b[0m, in \u001b[0;36mface_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39mReturns an array of bounding boxes of human faces in a image\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m:return: A list of tuples of found face locations in css (top, right, bottom, left) order\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcnn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m [_trim_css_to_bounds(_rect_to_css(face\u001b[39m.\u001b[39mrect), img\u001b[39m.\u001b[39mshape) \u001b[39mfor\u001b[39;00m face \u001b[39min\u001b[39;00m _raw_face_locations(img, number_of_times_to_upsample, \u001b[39m\"\u001b[39;49m\u001b[39mcnn\u001b[39;49m\u001b[39m\"\u001b[39;49m)]\n\u001b[1;32m    120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m [_trim_css_to_bounds(_rect_to_css(face), img\u001b[39m.\u001b[39mshape) \u001b[39mfor\u001b[39;00m face \u001b[39min\u001b[39;00m _raw_face_locations(img, number_of_times_to_upsample, model)]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/face_recognition/lib/python3.10/site-packages/face_recognition/api.py:103\u001b[0m, in \u001b[0;36m_raw_face_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39mReturns an array of bounding boxes of human faces in a image\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m:return: A list of dlib 'rect' objects of found face locations\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcnn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m cnn_face_detector(img, number_of_times_to_upsample)\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m face_detector(img, number_of_times_to_upsample)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "img_dirs =  list(paths.list_images(args[\"dataset\"]))\n",
    "known_encodings = []\n",
    "known_names = []\n",
    "\n",
    "for idx, img_dir in enumerate(img_dirs):\n",
    "    progress_per = round(idx/len(img_dirs)*100,2)\n",
    "    name = img_dir.split(os.path.sep)[-2]\n",
    "    if progress_per % 5 == 0:\n",
    "        print(f\"Progress: {progress_per}% | Encoded images: {idx+1}/{len(img_dirs)} | Name: {name}\")    # Load the image and convert to RGB (dlib ordering is RGB)\n",
    "    img = cv2.imread(img_dir)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Detect the (x,y) - coordinates to the bounding boxes\n",
    "    boxes = face_recognition.face_locations(img_rgb, model=args[\"detection_method\"])\n",
    "    # Compute the facial embedding for the face\n",
    "    encodings = face_recognition.face_encodings(img_rgb, boxes)\n",
    "    # Loop over the encodings\n",
    "    for encoding in encodings:\n",
    "        known_encodings.append(encoding)\n",
    "        known_names.append(name)\n",
    "print(\"\\nDone! Completed encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump\n",
    "print(\"Serializing...\")\n",
    "data = {\"encodings\": known_encodings, \"names\": known_names}\n",
    "f = open(\"encodings.pickle\", \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "34c5c970a2dc5bddc4259de08900cafaf4a9d097228d4e326234b4cfcb93435f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
